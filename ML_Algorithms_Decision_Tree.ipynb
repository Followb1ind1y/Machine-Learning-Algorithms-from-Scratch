{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM8sl29aemBgWlQAMmcO/m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Followb1ind1y/Machine_Learning_Algorithms/blob/main/ML_Algorithms_Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Algorithms: Decision Tree**"
      ],
      "metadata": {
        "id": "6NwsInh6N_Wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Decision Tree Basics**\n",
        "\n",
        "A **Decision Tree** is a non-parametric **supervised learning** algorithm. A tree has many analogies in real life, and turns out that it has influenced a wide area of machine learning, covering both **classification and regression**. Decision tree learning employs a **divide and conquer strategy** by conducting a **greedy search** to identify the optimal split points within a tree. This process of splitting is then repeated in a top-down, recursive manner until all, or the majority of records have been classified under specific class labels.\n",
        "\n",
        "**Decision Tree consists of:**\n",
        "\n",
        "* **Nodes :** Test for the value of a certain attribute.\n",
        "* **Edges/ Branch :** Correspond to the outcome of a test and connect to the next node or leaf.\n",
        "* **Leaf nodes :** Terminal nodes that predict the outcome (represent class labels or class distribution)."
      ],
      "metadata": {
        "id": "z5FYgIx0Of2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Splitting Criteria**"
      ],
      "metadata": {
        "id": "BjRA2XPGh7FF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reduction in Variance**\n",
        "\n",
        "**Reduction in Variance** is a method for splitting the node used when the target variable is **continuous**, i.e., **regression problems**. It is so-called because it uses variance as a measure for deciding the feature on which node is split into child nodes.\n",
        "\n",
        "$$\\mathrm{Variance} = \\frac{\\sum(X-\\mu)^{2}}{N}$$\n",
        "\n",
        "Variance is used for calculating the homogeneity of a node. If a node is entirely homogeneous, then the variance is zero."
      ],
      "metadata": {
        "id": "hNnAP5bohXJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sum of Squared Error (SSE)**\n",
        "\n",
        "The **Sum of Squared Error** is the most widely used splitting metric for **regression**. Suppose you want to divide the data set $S$ into two groups of $S_{1}$ and $S_{2}$, where the selection of $S_{1}$ and $S_{2}$ needs to minimize the sum of squared errors:\n",
        "\n",
        "$$\\mathrm{SSE} = \\sum_{i \\in S_{1}}(y_{i}-ȳ_{1})^{2}+\\sum_{i \\in S_{2}}(y_{i}-ȳ_{2})^{2}$$\n",
        "\n",
        "The way regression tree grows is to automatically decide on the splitting variables and split points that can **maximize SSE reduction**. Since this process is essentially a recursive segmentation, this approach is also called **recursive partitioning**."
      ],
      "metadata": {
        "id": "ucfe5kYBdXtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Information Gain (IG)**\n",
        "\n",
        "**Information Gain** is used for splitting the nodes when the target variable is categorical. It works on the concept of the entropy and is given by:\n",
        "\n",
        "$$\\mathrm{Information \\ Gain} = 1 - \\mathrm{Entroy}$$\n",
        "\n",
        "where $\\mathrm{Entropy}$ is a measure of the degree of disorder which is defined as:\n",
        "\n",
        "$$\\mathrm{Entroy} = -\\sum_{i}^{n}p_{i}\\log_{2}p_{i}$$\n",
        "\n",
        "$p_{i}$ is the probability of class $i$ and the interval of entropy is $[0,1]$. For a two-class problem:\n",
        "\n",
        "$$\\mathrm{Entroy} = -p\\log_{2}p-(1-p)\\log_{2}(1-p)$$\n",
        "\n",
        "**Lower the value of entropy, higher is the purity of the node**. The entropy of a homogeneous node is zero. Since we subtract entropy from 1, the **Information Gain is higher for the purer nodes** with a maximum value of 1."
      ],
      "metadata": {
        "id": "YDshRQDgaCRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gini Impurity**\n",
        "\n",
        "**Gini Impurity** is a measure of non-homogeneity. It is widely used in classification tree. **Gini** is the probability of correctly labeling a randomly chosen element if it was randomly labeled according to the distribution of labels in the node. The formula for Gini is:\n",
        "\n",
        "$$\\mathrm{Gini} = \\sum_{i}p_{i}^{2}$$\n",
        "\n",
        "\n",
        "And **Gini Impurity** is defined as:\n",
        "\n",
        "$$\\mathrm{Gini \\ Impurity} = 1 - \\mathrm{Gini} = 1 - \\sum_{i}p_{i}^{2} = \\sum_{i}p_{i}(1-p_{i})$$\n",
        "\n",
        "where $p_{i}$ is the probability of class $i$ and the interval of Gini is $[0,0.5]$. For a two-class problem, the Gini impurity for a given node is:\n",
        "\n",
        "$$p_{1}(1-p_{1}) + p_{2}(1-p_{2})$$\n",
        "\n",
        "It is easy to see that when the sample set is **pure**, one of the probability is 0 and the Gini score is the **smallest**. Conversely, when $p_{1} = p_{2} = 0.5$ the Gini score is the **largest**, in which case the purity of the node is the smallest."
      ],
      "metadata": {
        "id": "R61bs8WLQFQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tree Pruning**\n",
        "\n",
        "**Pruning** is the process that reduces the size of decision trees. It **reduces the risk of overfitting** by limiting the size of the tree or removing sections of the tree that provide little power.\n",
        "\n",
        "### **Limit the Size**\n",
        "We can limit the tree size by setting some parameters.\n",
        "\n",
        "* **Minimum sample size at each node**: Defining the minimum sample size at the node helps to prevent the leaf nodes having only one sample.\n",
        "* **Maximum depth of the tree**: If the tree grows too deep, the model tends to over-fit.\n",
        "* **Maximum number of terminal nodes**: Limit on the terminal nodes works the same as the limit on the depth of the tree.\n",
        "* **The number of variables considered for each split**: the algorithm randomly selects variables used in finding the optimal split point at each level. In general, the square root of the number of all variables works best, which is also the default setting for many functions."
      ],
      "metadata": {
        "id": "OHpBB2bvftR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove Branches**\n",
        "\n",
        "Another way is to first let the tree grow **as much as possible** and then go back to remove insignificant branches. The process reduces the depth of the tree. The idea is to overfit the training set and then correct using cross-validation.\n"
      ],
      "metadata": {
        "id": "V3YSXv8xgjhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost/Complexity Penalty**\n",
        "\n",
        "The idea is that the pruning minimizes the penalized error $\\mathrm{SSE}_{\\alpha}$ with a certain value of tuning parameter $\\alpha$.\n",
        "\n",
        "$$\\mathrm{SSE}_{\\alpha} = \\mathrm{SSE} + \\alpha \\cdot \\mathrm{complexity}$$\n",
        "\n",
        "Here complexity is a function of the number of leaves. For every given $\\alpha$, we want to find the tree that minimizes this penalized error."
      ],
      "metadata": {
        "id": "4JYZOiz3nV2T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7sLHWUbOC83"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}