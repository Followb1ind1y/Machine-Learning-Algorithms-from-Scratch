{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjU04jhe0yryHxIcxXpV+o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Followb1ind1y/Machine_Learning_Algorithms/blob/main/ML_Algorithms_Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Algorithms: Boosting**"
      ],
      "metadata": {
        "id": "5FVSO6-660i8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Boosting Basics**\n",
        "\n",
        "**Boosting** is an ensemble modeling technique which attempts to build a strong classifier from the number of **weak classifiers**. It is done by building a model using weak models in series. First, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added.\n",
        "\n",
        "Boosting being a sequential process, **each subsequent model attempts to correct the errors of the previous model**. It is focused on reducing the bias unlike bagging. It makes the boosting algorithms **prone to overfitting**. To avoid overfitting, parameter tuning plays an important role in boosting algorithms.\n",
        "\n",
        "A **Weak Classifier** is one whose error rate is only **slightly better than random guessing**. Theoretically a weak classifier can be boosted to perform pretty well. To find weak learners, we apply base learning (ML) algorithms with a different distribution. As each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n"
      ],
      "metadata": {
        "id": "ZKguoN-969xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Boosting Algorithm**\n",
        "\n",
        "1. Initialize all weights to $w_{i}=\\frac{1}{n}$ where $n$ is the number of instances in the dataset.\n",
        "\n",
        "2. For $i=1$ to $m$,\n",
        "    * a) Create a model and get the hypothesis $h_{j}(x) = \\arg \\min L_{j}$ that minimize $L_{j}=\\frac{\\sum_{i=1}^{n}w_{i}I(y_{i}\\neq h(x_{i}))}{\\sum_{i=1}^{n}w_{i}}$\n",
        "    * b) Compute $\\alpha_{j}=log(\\frac{1-L_{j}}{L_{j}})$\n",
        "    * c) Set $w_{i} := w_{i}e^{\\alpha_{j}I(y_{i}\\neq h(x_{i}))}$\n",
        "\n",
        "*  If classified correctly, the weight of an observation remains unchanged.\n",
        "*  If classified incorrectly, the weight is increased by multiplying\n",
        "$e^{\\alpha_{j}}$\n",
        "* Alpha varies with the degree of misclassification"
      ],
      "metadata": {
        "id": "iTuvFseY-kia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Final classification\n",
        "\n",
        "$$\n",
        "h(x_{i}) =sign[\\sum_{j=1}^{m}\\alpha_{j}h_{j}(x)]\n",
        "$$"
      ],
      "metadata": {
        "id": "U3v1lOE6CUvR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29NE4hb2CVDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}